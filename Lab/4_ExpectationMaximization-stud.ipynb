{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan>  Expectation Maximization for latent data models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where we are interested in estimating unknown parameters $\\theta\\in\\mathbb{R}^m$ characterizing a model with missing data, the Expectation Maximization (EM) algorithm (Dempster et al. 1977) can be used when the joint distribution of the missing data $X$ and the observed data $Y$ is explicit. For all $\\theta\\in\\mathbb{R}^m$, let $p_{\\theta}$ be the probability density function of $(X,Y)$ when the model is parameterized by $\\theta$ with respect to a given reference measure $\\mu$. The EM algorithm aims at computing iteratively an approximation of the maximum likelihood estimator which maximizes the observed data loglikelihood:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta;Y) = \\log p_{\\theta}(Y) =\\log \\int p_{\\theta}(x,Y)\\mu(\\mathrm{d}x)\\,.\n",
    "$$\n",
    "\n",
    "As this quantity cannot be computed explicitly in general cases, the EM algorithm finds the maximum likelihood estimator by iteratively maximizing the expected complete data loglikelihood.\n",
    " \n",
    "Start with an inital value $\\theta^{(0)}$ and let $\\theta^{(t)}$ be the estimate at the $t$-th iteration for $t\\geqslant 0$, then the next iteration of EM is decomposed into two steps.\n",
    "\n",
    "1. E-step. Compute the expectation of the complete data loglikelihood, with respect to the conditional distribution of the missing data given the observed data parameterized by $\\theta^{(t)}$:\n",
    "\n",
    "$$\n",
    "Q(\\theta,\\theta^{(t)}) =\\mathbb{E}_{\\theta^{(t)}}\\left[\\log p_{\\theta}(X,Y)|Y \\right]\\,.\n",
    "$$\n",
    "\n",
    "2. M step. Determine $\\theta^{(t+1)}$ by maximizing the function Q:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)}\\in \\mbox{argmax}_\\theta Q(\\theta,\\theta^{(t)})\\,.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 \n",
    "Prove the following crucial property motivates the EM algorithm.  \n",
    "For all $\\theta,\\theta^{(t)}$,\n",
    "    \n",
    "$$\n",
    "\\ell(Y;\\theta) - \\ell(Y;\\theta^{(t)}) \\geqslant Q(\\theta,\\theta^{(t)})-Q(\\theta^{(t)},\\theta^{(t)})\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, $X = (X_1,\\ldots,X_n)$ and $Y = (Y_1,\\ldots,Y_n)$ where $\\{(X_i,Y_i)\\}_{1\\leqslant i\\leqslant n}$  are i.i.d. in $\\{-1,1\\} \\times \\mathbb{R}^d$. For $k\\in\\{-1,1\\}$, write $\\pi_k = \\mathbb{P}(X_1 = k)$. Assume that, conditionally on the event $\\{X_1 = k\\}$, $Y_1$ has a Gaussian distribution with mean $\\mu_k \\in\\mathbb{R}^d$ and covariance matrix $\\Sigma\\in \\mathbb{R}^{d\\times d}$. In this case, the parameter $\\theta=(\\pi_1, \\mu_1,\\mu_{-1}, \\Sigma)$ belongs to the set $\\Theta= [0,1] \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\mathbb{R}^{d \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "- Write the complete data loglikelihood\n",
    "- Let $\\theta^{(t)}$ be the current parameter estimate. Compute $\\theta\\mapsto Q(\\theta,\\theta^{(t)})$.\n",
    "- Compute $\\theta^{(t+1)} = \\mathrm{argmax}_\\theta Q(\\theta,\\theta^{(t)})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange>  Simulated data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "- Sample data from a mixture of two Gaussian distributions and display the associated histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations\n",
    "n_samples = 100\n",
    "\n",
    "# means and variance to be estimated\n",
    "mu1, sigma1 = -2, 1.5\n",
    "mu2, sigma2 = 3, 1\n",
    "\n",
    "# prior probability of the first cluster/goup \n",
    "pi1 = 0.3\n",
    "\n",
    "# A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_pdf(x, mean, variance):\n",
    "  z = np.exp(-(x - mean)**2/(2*variance))/np.sqrt(2*np.pi*variance)\n",
    "  return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the training data\n",
    "\n",
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange>  EM algorithm</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot estimated density - plot the density of a mixture of two Gaussian distributions\n",
    "\n",
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    " - Compute $\\omega_t^i = \\mathbb{P}_{\\theta^{(t)}}(X_i=1|Y_i)$.\n",
    " - Write the loop of the EM algorithm.\n",
    " - Run the algorithm and display the loglikelihood and the estimates along iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clust = 2\n",
    "weights = np.ones((n_clust)) / n_clust\n",
    "means = [0, 1]\n",
    "variances = [0.5, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations of the EM algorithm\n",
    "n_it = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi1_est = np.zeros(n_it+1)\n",
    "mu1_est = np.zeros(n_it+1)\n",
    "mu2_est = np.zeros(n_it+1)\n",
    "sigma1_est = np.zeros(n_it+1)\n",
    "sigma2_est = np.zeros(n_it+1)\n",
    "\n",
    "pi1_est[0] = weights[0]\n",
    "mu1_est[0] = means[0]\n",
    "mu2_est[0] = means[1]\n",
    "sigma1_est[0] = variances[0]\n",
    "sigma2_est[0] = variances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loglikelihood along iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot parameters along iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "- Explore the sensitivity of the algorithm with respect to i) the initial estimates, ii) the number of iterations, iii) the number of observations and iv) the number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
