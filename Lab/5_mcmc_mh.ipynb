{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwUwrL3Vs6YA"
   },
   "source": [
    "## <font color=darkcyan> Markov chain and Markov chain Monte Carlo methods</font>\n",
    "$\n",
    "\\newcommand{\\PP}{\\mathbb P}\n",
    "\\newcommand{\\PE}{\\mathbb E}\n",
    "\\newcommand{\\Xset}{\\mathsf{X}}\n",
    "\\newcommand{\\nset}{\\mathbb{N}}\n",
    "\\newcommand{\\invcdf}[1]{F_{#1}^{\\leftarrow}}\n",
    "\\newcommand{\\rmd}{\\mathrm{d}}\n",
    "\\newcommand{\\rme}{\\mathrm{e}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yym8_tdfT3nu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as npr\n",
    "from math import pi\n",
    "\n",
    "# package which differentiates standard Python and Numpy code\n",
    "import autograd.numpy as np   ## Attention ce n'est pas numpy std qu'on utilise ici\n",
    "from autograd import grad\n",
    "\n",
    "# to get progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0uyG-Ta11yX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <font color=darkorange> The invariant measure of a Markov chain </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GKS4wzw-e2w"
   },
   "source": [
    "\n",
    "#### Question 1\n",
    "\n",
    "\n",
    "\n",
    "Consider a Gaussian AR($1$) process, $X_t= \\mu + \\phi X_{t-1} + \\sigma Z_t$, where $(Z_t)_{t \\in \\nset}$ is an iid sequence of standard Gaussian random variables, independent of $X_0$. Assume that $|\\phi| < 1$. Show that the Gaussian distribution with mean $\\mu/(1-\\phi)$ and variance $\\sigma^2/(1-\\phi^2)$ is a stationary distribution of the Markov chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write $P(x,y) \\propto \\exp(-(y-\\mu-\\phi x)^2/(2\\sigma^2)) $ the density of the AR(1) kernel. Then, we can compute $\\pi P$ where $\\pi$ is the Gaussian probability density function with mean $\\tilde\\mu$ and variance $\\tilde\\sigma^2$. For all $y$,\n",
    "$$\n",
    "\\pi P(y) \\propto \\int  \\exp(-(x-\\tilde \\mu)^2/(2\\tilde\\sigma^2))\\exp(-(y-\\mu-\\phi x)^2/(2\\sigma^2))\\mathrm{d} x\\,.\n",
    "$$\n",
    "This integral can be computed explicitly and $\\pi P$ is a Gaussian probability density function. It is then enough to solve the equation $\\pi P = \\pi$.\n",
    "\n",
    "An alternative is to note that if $X_0$ is Gaussian with mean $\\mu_0$ and variance $\\sigma^2_0$, then $X_1$ is Gaussian with mean $\\phi\\mu_0 + \\mu$ and variance $\\sigma^2 + \\phi^2\\sigma_0^2$ as $X_0$ and $Z_1$ are independent. The distribution of $X_0$ is a statitionary distribution if and only if $\\mu_0 = \\phi\\mu_0 + \\mu$ and $\\sigma_0^2 = \\sigma^2 + \\phi^2\\sigma_0^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Illustrate this property with an histogram of the values taken by a single trajectory of the Markov chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "tcBNLyCn_VYn",
    "outputId": "e332598e-991d-499e-eb15-108075955734"
   },
   "outputs": [],
   "source": [
    "p,mu,phi,sig=10000,1,0.9,1\n",
    "mc=npr.rand(1)*np.ones(p)\n",
    "\n",
    "f=lambda x,m,sq: np.exp(-(x-m)**2/(2*sq))/np.sqrt(2*pi*sq)\n",
    "mc[0]=0\n",
    "\n",
    "for i in range(p-1):\n",
    "    mc[i+1]=mu+phi*mc[i]+sig*npr.randn()\n",
    "\n",
    "x=np.linspace(min(mc),max(mc),30)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(mc,bins=80,density=True,edgecolor=\"black\")\n",
    "ax.plot(x,f(x,mu/(1-phi),sig**2/(1-phi**2)),color=\"red\")\n",
    "ax.grid(True)\n",
    "plt.title(f\"Histogram of a trajectory of the MC. n={p}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUCIQn9ETl-R"
   },
   "source": [
    "#### <font color=darkorange> Symmetric Random Walk Metropolis Hasting algorithm </font>\n",
    "\n",
    "We now consider a target distribution which is the mixture of two Gaussian distributions, one centered at $a>0$ and the other one centered at $-a$. For all $x\\in\\mathbb{R}$, \n",
    "$$\n",
    "\\pi(x)=\\frac{1}{2}\\left(\\phi(x-a)+\\phi(x+a)\\right)=\\frac{1}{2} \\frac{\\rme^{-(x-a)^2/2}}{\\sqrt{2\\pi}}+\\frac{1}{2} \\frac{\\rme^{-(x+a)^2/2}}{\\sqrt{2\\pi}}\\,,\n",
    "$$\n",
    "where $\\phi$ is the density of the centered standard normal distribution. \n",
    "\n",
    "To target this distribution, we sample according to a Symmetric Random Walk Metropolis Hasting algorithm. Let $X_0$ be sampled with an arbitrary distribution. Then, for $k\\geq 0$, when the chain is at the state $X_k$, we propose a candidate $Y_{k+1}$ according to $Y_{k+1}=X_k+ \\sigma Z_k$ where $Z_k\\sim {\\mathcal N}(0,1)$ and then we accept $X_{k+1}=Y_{k+1}$ with probability $\\alpha(X_k,Y_{k+1})$, where $\\alpha(x,y)=\\frac{\\pi(y)}{\\pi(x)} \\wedge 1$. Otherwise, we set  $X_{k+1}=X_{k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "- Write the Symmetric Random Walk MH loop with target distribution $\\pi$.\n",
    "- Display the trajectory of the Markov chain.\n",
    "- Display the histogram of the Markov chain as long as the target density.\n",
    "- Compare empirically several choices of $a$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "p = 5000\n",
    "a = 3\n",
    "\n",
    "mc = npr.randn()*np.ones(p)\n",
    "target_dist = lambda x,a: (np.exp(-(x-a)**2/2)+np.exp(-(x+a)**2/2))/np.sqrt(8*pi)\n",
    "\n",
    "samples = []\n",
    "for i in range(p-1):\n",
    "    v = npr.randn()\n",
    "    alpha = target_dist(mc[i]+v,a)/target_dist(mc[i],a)\n",
    "    mc[i+1] = mc[i]\n",
    "    if (npr.rand()<alpha):\n",
    "        mc[i+1] += v\n",
    "\n",
    "s = np.cumsum(mc)/np.arange(1,p+1)\n",
    "x = np.linspace(min(mc),max(mc),100)\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(15,9))\n",
    "ax[0].cla()\n",
    "ax[0].set_ylim(0, 0.25)\n",
    "ax[0].hist(mc,bins = 40, density = True, edgecolor = \"black\")\n",
    "ax[0].plot(x,target_dist(x,a),color=\"red\")\n",
    "ax[0].set_title(\"Histogram of a trajectory of the Markov chain\")\n",
    "ax[1].cla()\n",
    "ax[1].set_xlim(0,p)\n",
    "ax[1].set_ylim(-6,6)\n",
    "ax[1].plot(mc)\n",
    "ax[1].set_title(\"Trajectory of the Markov chain\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gauss_logpdf(mu, sigma):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    mu: mean of the Gaussian distribution\n",
    "    sigma: covariance matrix of the Gaussian distribution\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    logp: loglikelihood\n",
    "    \"\"\"\n",
    "\n",
    "    def logp(x): \n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ----------\n",
    "        x: array of size nxd\n",
    "    \n",
    "        Outputs\n",
    "        -------\n",
    "        Gaussian logdensity evaluated on all n points\n",
    "        \"\"\"\n",
    "        d = mu.shape[0]\n",
    "        cst = d * np.log(2 * np.pi)\n",
    "        det = np.log(np.linalg.det(sigma))\n",
    "        diff = x-mu\n",
    "        quad_term = np.einsum('...i,ij,...j', diff, np.linalg.inv(sigma), diff)\n",
    "        \n",
    "        return -0.5 *(cst +  det + quad_term) \n",
    "    \n",
    "    return logp\n",
    "\n",
    "def mixture_logpdf(log_probs, weights):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    log_probs: loglikelihood of each term\n",
    "    weights: weights of the components of the mixture\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    logp: loglikelihood of the mixture\n",
    "    \"\"\"\n",
    "    \n",
    "    def logp(x):\n",
    "        log_marginals = np.array([log_probs[j](x) for j in range(len(log_probs))])\n",
    "        return np.log(np.einsum('i,i...', weights, np.exp(log_marginals)))\n",
    "    \n",
    "    return logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a solution for any target density when we know the opposite of the logdensity to sample from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MH_monte_carlo(n_samples, log_prob, initial_state, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return\n",
    "    log_prob: loglikelihood to sample from\n",
    "    initial_state: initial sample\n",
    "    step_size: standard deviation of the proposed moves\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples from the MCMC algorithm\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted\n",
    "    \"\"\"\n",
    "    initial_state = np.array(initial_state)\n",
    "    \n",
    "    samples  = [initial_state]\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_state.shape[:1]\n",
    "    \n",
    "    # random variable to sample proposed moves\n",
    "    epsilon = npr.normal(size = size)\n",
    "    \n",
    "    for noise in tqdm(epsilon):\n",
    "        \n",
    "        q_new = samples[-1] + step_size*noise\n",
    "       \n",
    "        # acceptance rate\n",
    "        old_log_p = log_prob(samples[-1]) \n",
    "        new_log_p = log_prob(q_new) \n",
    "        \n",
    "        if np.log(np.random.rand()) < new_log_p - old_log_p:\n",
    "            samples.append(q_new)\n",
    "            accepted.append(True)\n",
    "        else:\n",
    "            samples.append(np.copy(samples[-1]))\n",
    "            accepted.append(False)\n",
    "\n",
    "    return (np.array(samples[1:]),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = 2*np.ones(2)\n",
    "cov1 = np.array([[1., 0.5],\n",
    "                [0.5, 1.]])\n",
    "mu2 = -mu1\n",
    "cov2 = np.array([[1., -0.1],\n",
    "                [-0.1, 1.]])\n",
    "\n",
    "mu3 = np.array([-1.5, 2])\n",
    "cov3 = 0.8 * np.eye(2)\n",
    "\n",
    "log_p = mixture_logpdf([multi_gauss_logpdf(mu1, cov1), multi_gauss_logpdf(mu2, cov2), multi_gauss_logpdf(mu3, cov3)],[0.3,0.3,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HM algorithm with symmetric RW\n",
    "n_samples = 20000\n",
    "step_size = 0.2\n",
    "samples_MH, accepted_MH = MH_monte_carlo(n_samples, log_p, np.random.randn(2), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True mixture density values\n",
    "grid_lim = 6\n",
    "nb_points = 100\n",
    "\n",
    "xplot = np.linspace(-grid_lim, grid_lim, nb_points)\n",
    "yplot = np.linspace(-grid_lim, grid_lim, nb_points)\n",
    "Xplot, Yplot = np.meshgrid(xplot, yplot)\n",
    "\n",
    "XY = np.dstack((Xplot,Yplot))\n",
    "\n",
    "Zplot = np.exp(log_p(XY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Markov chain vs true density\n",
    "fig, ax = plt.subplots(figsize=(8,10))\n",
    "ax.grid(True, color=\"white\")\n",
    "pcm = ax.pcolormesh(Xplot, Yplot, Zplot, cmap = 'Blues')\n",
    "fig.colorbar(pcm,orientation='horizontal', label='True density values')\n",
    "ax.plot(samples_MH[:,0], samples_MH[:,1], '.', color='orange', alpha = 0.1, label = 'RW Metropolis-Hastings samples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trajectories\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "axs[0].plot(samples_MH[:,0], alpha = 0.5)\n",
    "axs[0].set_title('1st coordinate')\n",
    "axs[1].plot(samples_MH[:,1], alpha = 0.5, c='orange')\n",
    "axs[1].set_title('2nd coordinate')\n",
    "fig.suptitle(\"RW Metropolis-Hastings trajectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSGUdjYE6UM9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <font color=darkorange> Independent Metropolis Hasting algorithm </font>\n",
    "\n",
    "We again consider a target distribution which is a mixture of two Gaussian distributions, one centered at $a>0$ and the other one centered at $-a$ \n",
    "$$\n",
    "\\pi(x)=\\frac{1}{2}\\left(\\phi(x-a)+\\phi(x+a)\\right)=\\frac{1}{2} \\frac{\\rme^{-(x-a)^2/2}}{\\sqrt{2\\pi}}+\\frac{1}{2} \\frac{\\rme^{-(x+a)^2/2}}{\\sqrt{2\\pi}},\n",
    "$$\n",
    "where $\\phi$ is the density of the centered standard normal distribution. \n",
    "\n",
    "To target this distribution, we sample according to a Metropolis Hasting algorithm with independent proposal.  Let $X_0$ be sampled with an arbitrary distribution. Then, for $k\\geq 0$, when the chain is at the state $X_k$, we propose a candidate $Y_{k+1}$ according to $Y_{k+1}=Z_k$ where $Z_k\\sim {\\mathcal N}(\\theta,\\sigma^2)$ and then we accept $X_{k+1}=Y_{k+1}$ with probability $\\alpha(X_k,Y_{k+1})$, where $\\alpha(x,y)=\\frac{\\pi(y)q(x)}{\\pi(x)q(y)} \\wedge 1=\\frac{\\pi(y)/q(y)}{\\pi(x)/q(x)} \\wedge 1$ and $q$ is the density of the Gaussian distribution with mean $\\theta$ and variance $\\sigma^2$. Otherwise, we set $X_{k+1}=X_{k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "- Write the independent MH loop with target distribution $\\pi$.\n",
    "- Display the trajectory of the Markov chain.\n",
    "- Display the histogram of the Markov chain as long as the target density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MH_independent(n_samples, log_prob, initial_state, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return\n",
    "    log_prob: loglikelihood to sample from\n",
    "    initial_state: initial sample\n",
    "    step_size: standard deviation of the proposed moves\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples from the MCMC algorithm\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted\n",
    "    \"\"\"\n",
    "    initial_state = np.array(initial_state)\n",
    "    \n",
    "    samples  = [initial_state]\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_state.shape[:1]\n",
    "    \n",
    "    # random variable to sample proposed moves\n",
    "    epsilon = npr.normal(size = size)\n",
    "    \n",
    "    for noise in tqdm(epsilon):\n",
    "        \n",
    "        q_new = step_size*noise\n",
    "       \n",
    "        # acceptance rate\n",
    "        old_log_p = log_prob(samples[-1]) \n",
    "        new_log_p = log_prob(q_new) \n",
    "        \n",
    "        if np.log(np.random.rand()) < (new_log_p-old_log_p) + 0.5*(np.dot(samples[-1],samples[-1]) - np.dot(q_new,q_new))/step_size**2:\n",
    "            samples.append(q_new)\n",
    "            accepted.append(True)\n",
    "        else:\n",
    "            samples.append(np.copy(samples[-1]))\n",
    "            accepted.append(False)\n",
    "\n",
    "    return (np.array(samples[1:]),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HM algorithm with independent proposals\n",
    "n_samples = 20000\n",
    "step_size = 1\n",
    "samples_ind, accepted_ind = MH_independent(n_samples, log_p, np.random.randn(2), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Markov chain vs true density\n",
    "fig, ax = plt.subplots(figsize=(8,10))\n",
    "ax.grid(True, color=\"white\")\n",
    "pcm = ax.pcolormesh(Xplot, Yplot, Zplot, cmap = 'Blues')\n",
    "fig.colorbar(pcm,orientation='horizontal', label='True density values')\n",
    "ax.plot(samples_ind[:,0], samples_ind[:,1], '.', color='orange', alpha = 0.1, label = 'RW Metropolis-Hastings samples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trajectories\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "axs[0].plot(samples_ind[:,0], alpha = 0.5)\n",
    "axs[0].set_title('1st coordinate')\n",
    "axs[1].plot(samples_ind[:,1], alpha = 0.5, c='orange')\n",
    "axs[1].set_title('2nd coordinate')\n",
    "fig.suptitle(\"Independent Metropolis-Hastings trajectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange> Metropolis Adjusted Langevin algorithm (MALA) </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Objective target density:`` $\\pi$.\n",
    "\n",
    "At each iteration $k\\geqslant 0$, generate $Z_{k+1} \\sim X_k + \\frac{\\sigma}{2}\\nabla\\log\\pi(X_k) + \\sigma \\varepsilon_{k+1}$.\n",
    "\n",
    "Set $X_{k+1} = Z_{k+1}$ with probability $\\alpha(X_k,Z_{k+1})$ and  $X_{k+1} = X_k$ with probability $1-\\alpha(X_k,Z_{k+1})$, where \n",
    "\n",
    "$$\n",
    "\\alpha(x,y) = 1\\wedge\\frac{\\pi(y)}{\\pi(x)}\\frac{q(y,x)}{q(x,y)}\\,,\n",
    "$$\n",
    "\n",
    "where $y\\mapsto q(x,y)$ is the Gaussian pdf with mean $x + \\frac{\\sigma}{2}\\nabla\\log\\pi(x)$ and variance $\\sigma^2 I_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "- Write the Metropolis Adjusted Langevin algorithm loop with target distribution $\\pi$.\n",
    "- Display the trajectory of the Markov chain.\n",
    "- Display the histogram of the Markov chain as long as the target density.\n",
    "- Compare empirically several choices of $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MALA_monte_carlo(n_samples, log_prob, initial_state, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return\n",
    "    log_prob: opposite of the loglikelihood to sample from\n",
    "    initial_state: initial sample\n",
    "    step_size: standard deviation of the proposed moves\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples from the MCMC algorithm\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted\n",
    "    \"\"\"\n",
    "    initial_state = np.array(initial_state)\n",
    "\n",
    "    gradV = grad(log_prob)\n",
    "\n",
    "    samples  = [initial_state]\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_state.shape[:1]\n",
    "    \n",
    "    # random variable to sample proposed moves\n",
    "    epsilon = npr.normal(size = size)\n",
    "    \n",
    "    for noise in tqdm(epsilon):\n",
    "        \n",
    "        grad_new = gradV(samples[-1])\n",
    "        mean_new = samples[-1] - 0.5*step_size*grad_new\n",
    "        \n",
    "        q_new    = mean_new + step_size*noise\n",
    "       \n",
    "        grad_y   = gradV(q_new)\n",
    "        mean_y   = q_new - 0.5*step_size*grad_y\n",
    "        \n",
    "        # acceptance rate\n",
    "        old_log_p = -log_prob(samples[-1]) + 0.5*np.dot(q_new-mean_new,q_new-mean_new)/(step_size**2)\n",
    "        new_log_p = -log_prob(q_new) + 0.5*np.dot(samples[-1]-mean_y,samples[-1]-mean_y)/(step_size**2)\n",
    "        \n",
    "        if np.log(np.random.rand()) < old_log_p - new_log_p:\n",
    "            samples.append(q_new)\n",
    "            accepted.append(True)\n",
    "        else:\n",
    "            samples.append(np.copy(samples[-1]))\n",
    "            accepted.append(False)\n",
    "\n",
    "    return (np.array(samples[1:]),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Metropolis Adjusted Langevin algorithm\n",
    "n_samples = 20000\n",
    "step_size = 0.2\n",
    "samples_Mala, accepted_Mala = MALA_monte_carlo(n_samples, log_p, np.random.randn(2), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Markov chain vs true density\n",
    "fig, ax = plt.subplots(figsize=(8,10))\n",
    "ax.grid(True, color=\"white\")\n",
    "pcm = ax.pcolormesh(Xplot, Yplot, Zplot, cmap = 'Blues')\n",
    "fig.colorbar(pcm,orientation='horizontal', label='True density values')\n",
    "ax.plot(samples_Mala[:,0], samples_Mala[:,1], '.', color='orange', alpha = 0.1, label = 'MALA samples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trajectories\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "axs[0].plot(samples_Mala[:,0], alpha = 0.5)\n",
    "axs[0].set_title('1st coordinate')\n",
    "axs[1].plot(samples_Mala[:,1], alpha = 0.5, c='orange')\n",
    "axs[1].set_title('2nd coordinate')\n",
    "fig.suptitle(\"MALA trajectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "Compare all MH versions when the target $\\pi$ is a mixture of Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
