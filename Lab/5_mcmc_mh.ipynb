{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwUwrL3Vs6YA"
   },
   "source": [
    "## <font color=darkcyan> Markov chain and Markov chain Monte Carlo methods</font>\n",
    "$\n",
    "\\newcommand{\\PP}{\\mathbb P}\n",
    "\\newcommand{\\PE}{\\mathbb E}\n",
    "\\newcommand{\\Xset}{\\mathsf{X}}\n",
    "\\newcommand{\\nset}{\\mathbb{N}}\n",
    "\\newcommand{\\invcdf}[1]{F_{#1}^{\\leftarrow}}\n",
    "\\newcommand{\\rmd}{\\mathrm{d}}\n",
    "\\newcommand{\\rme}{\\mathrm{e}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yym8_tdfT3nu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import expon, geom, norm\n",
    "from math import pi\n",
    "\n",
    "import seaborn as sns\n",
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "# package which differentiates standard Python and Numpy code\n",
    "from autograd import grad\n",
    "# to get progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0uyG-Ta11yX"
   },
   "source": [
    "#### <font color=darkorange> The invariant measure of a Markov chain </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GKS4wzw-e2w"
   },
   "source": [
    "\n",
    "#### Question 1\n",
    "\n",
    "\n",
    "\n",
    "Consider a Gaussian AR($1$) process, $X_t= \\mu + \\phi X_{t-1} + \\sigma Z_t$, where $(Z_t)_{t \\in \\nset}$ is an iid sequence of standard Gaussian random variables, independent of $X_0$. Assume that $|\\phi| < 1$. Show that the Gaussian distribution with mean $\\mu/(1-\\phi)$ and variance $\\sigma^2/(1-\\phi^2)$ is a stationary distribution of the Markov chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write $P(x,y) \\propto \\exp(-(y-\\mu-\\phi x)^2/(2\\sigma^2)) $ the density of the AR(1) kernel. Then, we can compute $\\pi P$ where $\\pi$ is the Gaussian probability density function with mean $\\tilde\\mu$ and variance $\\tilde\\sigma^2$. For all $y$,\n",
    "$$\n",
    "\\pi P(y) \\propto \\int  \\exp(-(x-\\tilde \\mu)^2/(2\\tilde\\sigma^2))\\exp(-(y-\\mu-\\phi x)^2/(2\\sigma^2))\\mathrm{d} x\\,.\n",
    "$$\n",
    "This integral can be computed explicitly and $\\pi P$ is a Gaussian probability density function. It is then enough to solve the equation $\\pi P = \\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Illustrate this property with an histogram of the values taken by a single trajectory of the Markov chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "tcBNLyCn_VYn",
    "outputId": "e332598e-991d-499e-eb15-108075955734"
   },
   "outputs": [],
   "source": [
    "p,mu,phi,sig=10000,1,0.9,1\n",
    "mc=npr.rand(1)*np.ones(p)\n",
    "\n",
    "f=lambda x,m,sq: np.exp(-(x-m)**2/(2*sq))/np.sqrt(2*pi*sq)\n",
    "mc[0]=0\n",
    "\n",
    "for i in range(p-1):\n",
    "    mc[i+1]=mu+phi*mc[i]+sig*npr.randn()\n",
    "\n",
    "x=np.linspace(min(mc),max(mc),30)\n",
    "plt.hist(mc,bins=80,density=True,edgecolor=\"black\")\n",
    "plt.plot(x,f(x,mu/(1-phi),sig**2/(1-phi**2)),color=\"red\")\n",
    "plt.title(\"Histogram of a trajectory of the MC. n=\"+str(p))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUCIQn9ETl-R"
   },
   "source": [
    "#### <font color=darkorange> Symmetric Random Walk Metropolis Hasting algorithm </font>\n",
    "\n",
    "We now consider a target distribution which is the mixture of two Gaussian distributions, one centered at $a$ and the other one centered at $-a$ \n",
    "$$\n",
    "\\pi(x)=\\frac{1}{2}\\left(\\phi(x-a)+\\phi(x+a)\\right)=\\frac{1}{2} \\frac{\\rme^{-(x-a)^2}}{\\sqrt{2\\pi}}+\\frac{1}{2} \\frac{\\rme^{-(x+a)^2}}{\\sqrt{2\\pi}}\n",
    "$$\n",
    "where $\\phi$ is the density of the centered standard normal distribution. \n",
    "\n",
    "To target this distribution, we sample according to a Symmetric Random Walk Metropolis Hasting algorithm. When the chain is at the state $X_k$, we propose a candidate $Y_{k+1}$ according to $Y_{k+1}=X_k+ \\sigma Z_k$ where $Z_k\\sim {\\mathcal N}(0,1)$ and then we accept $X_{k+1}=Y_{k+1}$ with probability $\\alpha(X_k,Y_{k+1})$, where $\\alpha(x,y)=\\frac{\\pi(y)}{\\pi(x)} \\wedge 1$. Otherwise, $X_{k+1}=X_{k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "- Write the Symmetric Random Walk MH loop with target distribution $\\pi$.\n",
    "- Display the trajectory of the Markov chain.\n",
    "- Display the histogram of the Markov chain as long as the target density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to define the target distribution in particular for mixtures of Gaussian distributions.\n",
    "# Can be replaced by a simpler target distributions as proposed.\n",
    "\n",
    "def multi_gauss(mu, sigma):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    mu: mean of the Gaussian distribution\n",
    "    sigma: covariance matrix of the Gaussian distribution\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    logp: opposite of the loglikelihood\n",
    "    \"\"\"\n",
    "\n",
    "    def logp(x):\n",
    "        k   = mu.shape[0]\n",
    "        cst       = k * np.log(2 * np.pi)\n",
    "        det       = np.log(np.linalg.det(sigma))\n",
    "        quad_term = np.dot(np.dot((x - mu).T, np.linalg.inv(sigma)), x - mu)\n",
    "        return (cst +  det + quad_term) * 0.5\n",
    "    \n",
    "    return logp\n",
    "\n",
    "def mixture(log_prob, weights):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    log_prob: opposite of the likelihood of each term\n",
    "    weights: weights of the components of the mixture\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    logp: opposite of the loglikelihood of the mixture\n",
    "    \"\"\"\n",
    "    \n",
    "    def logp(x):\n",
    "        likelihood = 0\n",
    "        for j in range(np.size(weights)):\n",
    "            log_marginal = -log_prob[j](x)\n",
    "            likelihood   = likelihood + weights[j]*np.exp(log_marginal)\n",
    "        \n",
    "        return -np.log(likelihood)\n",
    "\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lim = 6\n",
    "# grid on which the target pdf is displayed\n",
    "grid_plot = (-grid_lim, grid_lim, -grid_lim, grid_lim)\n",
    "# coordinates chosen on this grid\n",
    "nb_points = 100\n",
    "\n",
    "xplot = np.linspace(-grid_lim, grid_lim, nb_points)\n",
    "yplot = np.linspace(-grid_lim, grid_lim, nb_points)\n",
    "Xplot, Yplot = np.meshgrid(xplot, yplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a solution for any target density when we know the opposite of the logdensity to sample from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MH_monte_carlo(n_samples, log_prob, initial_state, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return\n",
    "    log_prob: opposite of the loglikelihood to sample from\n",
    "    initial_state: initial sample\n",
    "    step_size: standard deviation of the proposed moves\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples from the MCMC algorithm\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted\n",
    "    \"\"\"\n",
    "    initial_state = np.array(initial_state)\n",
    "    \n",
    "    samples  = [initial_state]\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_state.shape[:1]\n",
    "    \n",
    "    # random variable to sample proposed moves\n",
    "    epsilon = st.norm(0, 1).rvs(size)\n",
    "    \n",
    "    for noise in tqdm(epsilon):\n",
    "        \n",
    "        q_new = samples[-1] + step_size*noise\n",
    "       \n",
    "        # acceptance rate\n",
    "        old_log_p = log_prob(samples[-1]) \n",
    "        new_log_p = log_prob(q_new) \n",
    "        \n",
    "        if np.log(np.random.rand()) < old_log_p - new_log_p:\n",
    "            samples.append(q_new)\n",
    "            accepted.append(True)\n",
    "        else:\n",
    "            samples.append(np.copy(samples[-1]))\n",
    "            accepted.append(False)\n",
    "\n",
    "    return (np.array(samples[1:]),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = 2*np.ones(2)\n",
    "cov1 = np.array([[1., 0.5],\n",
    "                [0.5, 1.]])\n",
    "mu2 = -mu1\n",
    "cov2 = np.array([[1., -0.1],\n",
    "                [-0.1, 1.]])\n",
    "\n",
    "mu3 = np.array([-1.5, 2.2])\n",
    "cov3 = 0.8 * np.eye(2)\n",
    "\n",
    "log_p = mixture([multi_gauss(mu1, cov1), multi_gauss(mu2, cov2), multi_gauss(mu3, cov3)],[0.3,0.3,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20000\n",
    "step_size = 0.2\n",
    "samples_MH, accepted_MH = MH_monte_carlo(n_samples, log_p, np.random.randn(2), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "Zplot = np.copy(Xplot)\n",
    "for i in range(nb_points):\n",
    "    for j in range(nb_points):\n",
    "        Zplot[i][j] = np.exp(-log_p(np.array((Xplot[i][j], Yplot[i][j]))))\n",
    "\n",
    "plt.imshow(Zplot, alpha = 0.9, extent = grid_plot, cmap='Blues', origin='upper')\n",
    "plt.plot(samples_MH[:,0], samples_MH[:,1], '.', color='orange', alpha = 0.1, label = 'RW Metropolis-Hastings samples')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(samples_MH[:,0], alpha = 0.5, label = 'RW Metropolis-Hastings trajectory, 1st coordinate')\n",
    "plt.plot(samples_MH[:,1], alpha = 0.5, label = 'RW Metropolis-Hastings trajectory, 2nd coordinate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSGUdjYE6UM9"
   },
   "source": [
    "#### <font color=darkorange> Independent Metropolis Hasting algorithm </font>\n",
    "\n",
    "We again consider a target distribution which is a mixture of two Gaussian distributions, one centered at $a$ and the other one centered at $-a$ \n",
    "$$\n",
    "\\pi(x)=\\frac{1}{2}\\left(\\phi(x-a)+\\phi(x+a)\\right)=\\frac{1}{2} \\frac{\\rme^{-(x-a)^2}}{\\sqrt{2\\pi}}+\\frac{1}{2} \\frac{\\rme^{-(x+a)^2}}{\\sqrt{2\\pi}},\n",
    "$$\n",
    "where $\\phi$ is the density of the centered standard normal distribution. \n",
    "\n",
    "To target this distribution, we sample according to a Metropolis Hasting algorithm with independent proposal. When the chain is at the state $X_k$, we propose a candidate $Y_{k+1}$ according to $Y_{k+1}=Z_k$ where $Z_k\\sim {\\mathcal N}(\\theta,\\sigma^2)$ and then we accept $X_{k+1}=Y_{k+1}$ with probability $\\alpha(X_k,Y_{k+1})$, where $\\alpha(x,y)=\\frac{\\pi(y)q(x)}{\\pi(x)q(y)} \\wedge 1=\\frac{\\pi(y)/q(y)}{\\pi(x)/q(x)} \\wedge 1$ and $q$ is the density of ${\\mathcal N}(\\theta,\\sigma^2)$. Otherwise, $X_{k+1}=X_{k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "- Write the independent MH loop with target distribution $\\pi$.\n",
    "- Display the trajectory of the Markov chain.\n",
    "- Display the histogram of the Markov chain as long as the target density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MH_independent(n_samples, log_prob, initial_state, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return\n",
    "    log_prob: opposite of the loglikelihood to sample from\n",
    "    initial_state: initial sample\n",
    "    step_size: standard deviation of the proposed moves\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples from the MCMC algorithm\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted\n",
    "    \"\"\"\n",
    "    initial_state = np.array(initial_state)\n",
    "    \n",
    "    samples  = [initial_state]\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_state.shape[:1]\n",
    "    \n",
    "    # random variable to sample proposed moves\n",
    "    epsilon = st.norm(0, 1).rvs(size)\n",
    "    \n",
    "    for noise in tqdm(epsilon):\n",
    "        \n",
    "        q_new = step_size*noise\n",
    "       \n",
    "        # acceptance rate\n",
    "        old_log_p = log_prob(samples[-1]) \n",
    "        new_log_p = log_prob(q_new) \n",
    "        \n",
    "        if np.log(np.random.rand()) < (old_log_p - new_log_p) + 0.5*(np.dot(samples[-1],samples[-1]) - np.dot(q_new,q_new))/step_size**2:\n",
    "            samples.append(q_new)\n",
    "            accepted.append(True)\n",
    "        else:\n",
    "            samples.append(np.copy(samples[-1]))\n",
    "            accepted.append(False)\n",
    "\n",
    "    return (np.array(samples[1:]),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20000\n",
    "step_size = 1\n",
    "samples_ind, accepted_ind = MH_independent(n_samples, log_p, np.random.randn(2), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "Zplot = np.copy(Xplot)\n",
    "for i in range(nb_points):\n",
    "    for j in range(nb_points):\n",
    "        Zplot[i][j] = np.exp(-log_p(np.array((Xplot[i][j], Yplot[i][j]))))\n",
    "\n",
    "plt.imshow(Zplot, alpha = 0.9, extent = grid_plot, cmap='Blues', origin='upper')\n",
    "plt.plot(samples_ind[:,0], samples_ind[:,1], '.', color='orange', alpha = 0.1, label = 'RW Metropolis-Hastings samples')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(samples_ind[:,0], alpha = 0.5, label = 'RW Metropolis-Hastings trajectory, 1st coordinate')\n",
    "plt.plot(samples_ind[:,1], alpha = 0.5, label = 'RW Metropolis-Hastings trajectory, 2nd coordinate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange> Metropolis Adjusted Langevin algorithm (MALA) </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Objective target density:`` $\\pi$.\n",
    "\n",
    "At each iteration $k\\geqslant 0$, generate $Z_{k+1} \\sim X_k + \\frac{\\sigma}{2}\\nabla\\log\\pi(X_k) + \\sigma \\varepsilon_{k+1}$.\n",
    "\n",
    "Set $X_{k+1} = Z_{k+1}$ with probability $\\alpha(X_k,Z_{k+1})$ and  $X_{k+1} = X_k$ with probability $1-\\alpha(X_k,Z_{k+1})$, where \n",
    "\n",
    "$$\n",
    "\\alpha(x,y) = 1\\wedge\\frac{\\pi(y)}{\\pi(x)}\\frac{q(y,x)}{q(x,y)}\\,,\n",
    "$$\n",
    "\n",
    "where $q(x,y)$ is the Gaussian pdf with mean $x + \\frac{\\sigma}{2}\\nabla\\log\\pi(x)$ and variance $\\sigma^2 I_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "- Write the Metropolis Adjusted Langevin algorithm loop with target distribution $\\pi$.\n",
    "- Display the trajectory of the Markov chain.\n",
    "- Display the histogram of the Markov chain as long as the target density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MALA_monte_carlo(n_samples, log_prob, initial_state, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return\n",
    "    log_prob: opposite of the loglikelihood to sample from\n",
    "    initial_state: initial sample\n",
    "    step_size: standard deviation of the proposed moves\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples from the MCMC algorithm\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted\n",
    "    \"\"\"\n",
    "    initial_state = np.array(initial_state)\n",
    "\n",
    "    gradV = grad(log_prob)\n",
    "\n",
    "    samples  = [initial_state]\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_state.shape[:1]\n",
    "    \n",
    "    # random variable to sample proposed moves\n",
    "    epsilon = st.norm(0, 1).rvs(size)\n",
    "    \n",
    "    for noise in tqdm(epsilon):\n",
    "        \n",
    "        grad_new = gradV(samples[-1])\n",
    "        mean_new = samples[-1] - 0.5*step_size*grad_new\n",
    "        \n",
    "        q_new    = mean_new + step_size*noise\n",
    "       \n",
    "        grad_y   = gradV(q_new)\n",
    "        mean_y   = q_new - 0.5*step_size*grad_y\n",
    "        \n",
    "        # acceptance rate\n",
    "        old_log_p = log_prob(samples[-1]) + 0.5*np.dot(q_new-mean_new,q_new-mean_new)/(step_size**2)\n",
    "        new_log_p = log_prob(q_new) + 0.5*np.dot(samples[-1]-mean_y,samples[-1]-mean_y)/(step_size**2)\n",
    "        \n",
    "        if np.log(np.random.rand()) < old_log_p - new_log_p:\n",
    "            samples.append(q_new)\n",
    "            accepted.append(True)\n",
    "        else:\n",
    "            samples.append(np.copy(samples[-1]))\n",
    "            accepted.append(False)\n",
    "\n",
    "    return (np.array(samples[1:]),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = 2*np.ones(2)\n",
    "cov1 = np.array([[1., 0.5],\n",
    "                [0.5, 1.]])\n",
    "mu2 = -mu1\n",
    "cov2 = np.array([[1., -0.1],\n",
    "                [-0.1, 1.]])\n",
    "\n",
    "mu3 = np.array([-1.5, 2.2])\n",
    "cov3 = 0.8 * np.eye(2)\n",
    "\n",
    "log_p = mixture([multi_gauss(mu1, cov1), multi_gauss(mu2, cov2), multi_gauss(mu3, cov3)], [0.3, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20000\n",
    "step_size = 0.2\n",
    "samples_Mala, accepted_Mala = MALA_monte_carlo(n_samples, log_p, np.random.randn(2), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.imshow(Zplot, alpha = 0.9, extent = grid_plot, cmap='Blues', origin='upper')\n",
    "plt.plot(samples_Mala[:,0], samples_Mala[:,1], '.', color='orange', alpha = 0.1, label = 'MALA samples')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(samples_Mala[:,0], alpha = 0.5, label = 'MALA trajectory, 1st coordinate')\n",
    "plt.plot(samples_Mala[:,1], alpha = 0.5, label = 'MALA trajectory, 2nd coordinate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "Compare all MH versions when the target $\\pi$ is a mixture of Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
