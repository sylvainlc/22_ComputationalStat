{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan> MCMC algorithms</font>\n",
    "#### <font color=darkorange>Metropolis-Hastings, MALA,  Hamiltonian Monte Carlo... </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Required packages\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import seaborn as sns\n",
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "# package which differentiates standard Python and Numpy code\n",
    "from autograd import grad\n",
    "# to get progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Bayesian setting, a parameter $x$ is embedded with a prior distribution $\\pi$ and the observations are given by a probabilistic model:\n",
    "\n",
    "$$\n",
    "Y\\sim \\ell(\\cdot|x)\\,.\n",
    "$$\n",
    "\n",
    "\n",
    "The inference is then based on the posterior distribution:\n",
    "$$\n",
    "\\pi(x|Y) = \\frac{\\pi(x)\\ell(Y|x)}{\\int\\pi(u)\\ell(Y|u)\\mathrm{d} u}\\,.\n",
    "$$\n",
    "\n",
    "\n",
    "In most cases the normalizing constant is not tractable:\n",
    "$$\n",
    "\\pi(x|Y) \\propto \\pi(x)\\ell(Y|x)\\,.\n",
    "$$\n",
    "\n",
    "``Markov chain Monte Carlo (MCMC) algorithms`` provide solutions to sample from posterior distributions. ``Hamiltonian Monte Carlo (HMC)`` is a MCMC algorithm that uses gradient information to scale better to higher dimensions. It is used by software like [PyMC3](https://pymc.io/) and [Stan](https://mc-stan.org/). \n",
    "\n",
    "Some references on MCMC...\n",
    "- **Douc R., Moulines E. and Stoffer D.**, Nonlinear time series: theory, methods and applications with R example, 2014, Chapman \\& Hall.\n",
    "- **Michael Betancourt, [A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434)** A thorough, readable reference that is the main source here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange> A few simple models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the target density is written $\\pi$. We define below two examples of target densities $\\pi$ which will be used to assess the efficiency of the proposed MCMC algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Write a function returning the opposite of the log probability density of </font>\n",
    "    \n",
    "<font color=darkred>    i) a Gaussian random variable with mean mu and covariance matrix sigma; </font>\n",
    "    \n",
    "<font color=darkred>    ii) a mixture of probability density functions. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gauss(mu, sigma):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    mu: mean of the Gaussian distribution\n",
    "    sigma: covariance matrix of the Gaussian distribution\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    logp: opposite of the loglikelihood\n",
    "    \"\"\"\n",
    "\n",
    "    def logp(x):\n",
    "        \n",
    "    \n",
    "    return logp\n",
    "\n",
    "def mixture(log_prob, weights):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    log_prob: opposite of the likelihood of each term\n",
    "    weights: weights of the components of the mixture\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    logp: opposite of the loglikelihood of the mixture\n",
    "    \"\"\"\n",
    "    \n",
    "    def logp(x):\n",
    "        \n",
    "\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lim = 6\n",
    "# grid on which the target pdf is displayed\n",
    "grid_plot = (-grid_lim, grid_lim, -grid_lim, grid_lim)\n",
    "# coordinates chosen on this grid\n",
    "nb_points = 100\n",
    "\n",
    "xplot = np.linspace(-grid_lim, grid_lim, nb_points)\n",
    "yplot = np.linspace(-grid_lim, grid_lim, nb_points)\n",
    "Xplot, Yplot = np.meshgrid(xplot, yplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkcyan> Metropolis-Hastings algorithm </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Objective target density:`` $\\pi(\\cdot|Y)$.\n",
    "\n",
    "``Instrumental transition density:`` $q(x,y)$.\n",
    "\n",
    "At each iteration $k\\geqslant 0$, generate $Z_{k+1} \\sim q(X_k,\\cdot)$.\n",
    "\n",
    "Set $X_{k+1} = Z_{k+1}$ with probability $\\alpha(X_k,Z_{k+1})$ and  $X_{k+1} = X_k$ with probability $1-\\alpha(X_k,Z_{k+1})$, where \n",
    "\n",
    "$$\n",
    "\\alpha(x,y) = 1\\wedge\\frac{\\pi(y|Y)}{\\pi(x|Y)}\\frac{q(y,x)}{q(x,y)}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Write a function which returns samples from Metropolis-Hastings algorithm with Gaussian proposal density.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HM_monte_carlo(n_samples, log_prob, initial_state, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return\n",
    "    log_prob: opposite of the loglikelihood to sample from\n",
    "    initial_state: initial sample\n",
    "    step_size: standard deviation of the proposed moves\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples from the MCMC algorithm\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted\n",
    "    \"\"\"\n",
    "    initial_state = np.array(initial_state)\n",
    "    \n",
    "    samples  = [initial_state]\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_state.shape[:1]\n",
    "    \n",
    "    # random variable to sample proposed moves\n",
    "    epsilon = st.norm(0, 1).rvs(size)\n",
    "    \n",
    "    for noise in tqdm(epsilon):\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        \n",
    "\n",
    "    return (np.array(samples[1:]),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkcyan> Metropolis Adjusted Langevin algorithm (MALA) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Objective target density:`` $\\pi(\\cdot|Y)$.\n",
    "\n",
    "At each iteration $k\\geqslant 0$, generate $Z_{k+1} \\sim X_k + \\frac{\\sigma^2}{2}\\nabla\\log\\pi(X_k|Y) + \\sigma \\varepsilon_{k+1}$.\n",
    "\n",
    "Set $X_{k+1} = Z_{k+1}$ with probability $\\alpha(X_k,Z_{k+1})$ and  $X_{k+1} = X_k$ with probability $1-\\alpha(X_k,Z_{k+1})$, where \n",
    "\n",
    "$$\n",
    "\\alpha(x,y) = 1\\wedge\\frac{\\pi(y|Y)}{\\pi(x|Y)}\\frac{q(y,x)}{q(x,y)}\\,,\n",
    "$$\n",
    "\n",
    "where $q(x,y)$ is the Gaussian pdf with mean $x + \\frac{\\sigma^2}{2}\\nabla\\log\\pi(x|Y)$ and variance $\\sigma^2 I_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Write a function which returns samples from MALA algorithm.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MALA_monte_carlo(n_samples, log_prob, initial_state, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return\n",
    "    log_prob: opposite of the loglikelihood to sample from\n",
    "    initial_state: initial sample\n",
    "    step_size: standard deviation of the proposed moves\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples from the MCMC algorithm\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted\n",
    "    \"\"\"\n",
    "    initial_state = np.array(initial_state)\n",
    "\n",
    "    gradV = grad(log_prob)\n",
    "\n",
    "    samples  = [initial_state]\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_state.shape[:1]\n",
    "    \n",
    "    # random variable to sample proposed moves\n",
    "    epsilon = st.norm(0, 1).rvs(size)\n",
    "    \n",
    "    for noise in tqdm(epsilon):\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        \n",
    "\n",
    "    return (np.array(samples[1:]),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkcyan> Hamiltonian Monte Carlo </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange> Framework </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unknown  parameters are gathered into a _position_ variable, usually written $\\mathbf{q}$. \n",
    "An auxiliary variable, called ``momentum`` and written $\\mathbf{p}$, is introduced to define the extended joint probability distribution \n",
    "\n",
    "$$\n",
    "\\pi(\\mathbf{q}, \\mathbf{p}|Y) =  \\pi(\\mathbf{q}|Y) \\pi(\\mathbf{p} | \\mathbf{q},Y)\\,,\n",
    "$$\n",
    "\n",
    "In most cases $\\pi(\\mathbf{p} | \\mathbf{q},Y)$ is the probability density of a Gaussian random variable with mean $0$ and variance $M$.\n",
    "\n",
    "The ``Hamiltonian`` associated with this model is  $H(\\mathbf{q}, \\mathbf{p}|Y) = -\\log \\pi(\\mathbf{q}, \\mathbf{p}|Y)$ so that\n",
    "\n",
    "$$\n",
    "H(\\mathbf{q}, \\mathbf{p}|Y) = -\\log \\pi(\\mathbf{p} | \\mathbf{q},Y) - \\log \\pi(\\mathbf{q}|Y) = K(\\mathbf{p}, \\mathbf{q}|Y) + V(\\mathbf{q}|Y)\\,,\n",
    "$$\n",
    "\n",
    "where $K(\\mathbf{p}, \\mathbf{q}|Y)$ is called the _kinetic energy_, and $V(\\mathbf{q}|Y)$ is called the _potential energy_.\n",
    "\n",
    "The dynamics of the system $(\\mathbf{q}, \\mathbf{p})$ is assumed to follow _Hamilton's equations_:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} \\mathbf{q}}{\\mathrm{d}t} = \\frac{\\partial H}{\\partial \\mathbf{p}} = \\frac{\\partial K}{\\partial \\mathbf{p}} + \\frac{\\partial V}{\\partial \\mathbf{p}}\\\\\n",
    "\\frac{\\mathrm{d} \\mathbf{p}}{\\mathrm{d}t} = -\\frac{\\partial H}{\\partial \\mathbf{q}}= -\\frac{\\partial K}{\\partial \\mathbf{q}} + \\frac{\\partial V}{\\partial \\mathbf{q}}\n",
    "$$\n",
    "\n",
    "\n",
    "When $\\pi(\\mathbf{p} | \\mathbf{q},Y)$ is the probability density of a Gaussian random variable with mean $0$ and variance $M$, this yields\n",
    "\n",
    "$$\n",
    "K(\\mathbf{p}, \\mathbf{q}) = \\frac{1}{2}\\mathbf{p}^T M^{-1}\\mathbf{p} + \\log |M| + \\text{C},\n",
    "$$\n",
    "\n",
    "In the specific case of $M=I$,  \n",
    "$$\n",
    "K(\\mathbf{p}, \\mathbf{q}) = \\frac{1}{2}\\mathbf{p}^T \\mathbf{p} + \\text{C}\\,,\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial K}{\\partial \\mathbf{p}} = \\mathbf{p} \\quad \\mathrm{and} \\quad \\frac{\\partial K}{\\partial \\mathbf{q}} = \\mathbf{0}\\,.\n",
    "$$\n",
    "\n",
    "\n",
    "Finally,\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} \\mathbf{q}}{\\mathrm{d}t}  = \\mathbf{p}\\quad \\mathrm{and} \\quad \\frac{\\mathrm{d} \\mathbf{p}}{\\mathrm{d}t} = - \\frac{\\partial V}{\\partial \\mathbf{q}}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Write a function returning the positions and momemtums of a Euler scheme based interator </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_integrator(q, p, gradientV, T, step):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    q: initial position\n",
    "    p: initial momentum\n",
    "    gradientV: gradient of the velocity\n",
    "    T: time horizon\n",
    "    step: step size to discretize the ODE\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    q, p: last position and last momentum\n",
    "    posisions: sequence of positions produced by the Euler based scheme integrator\n",
    "    momentums: sequence of momentums produced by the Euler based scheme integrator\n",
    "    \"\"\"\n",
    "    \n",
    "    q, p      = np.copy(q), np.copy(p)\n",
    "    pos, moms = [np.copy(q)], [np.copy(p)]\n",
    "\n",
    "    vq = gradientV(q)\n",
    "    nb_steps = int(T / step)\n",
    "    \n",
    "    for it in range(nb_steps):\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "\n",
    "    return q, -p, np.array(pos), np.array(moms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange> The Leapfrog integrator </font>\n",
    "The leapfrog integrator may be used to approximate the ordinary differential equations (ODE) $q$ and $p$ are solutions to. \n",
    "\n",
    "It involves updating the momentum `p` a half step, then the position `q` a whole step, and then finish updating `p` the other half of the step.\n",
    "\n",
    "A momentum flip at the end is required to preserve the reversibility of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Write a function returning the positions and momemtums of a leapfrog interator </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog_integrator(q, p, gradientV, T, step):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    q: initial position\n",
    "    p: initial momentum\n",
    "    gradientV: gradient of the velocity\n",
    "    T: time horizon\n",
    "    step: step size to discretize the ODE\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    q, p: last position and last momentum\n",
    "    posisions: sequence of positions produced by the leapfrog integrator\n",
    "    momentums: sequence of momentums produced by the leapfrog integrator\n",
    "    \"\"\"\n",
    "    \n",
    "    q, p      = np.copy(q), np.copy(p)\n",
    "    pos, moms = [np.copy(q)], [np.copy(p)]\n",
    "\n",
    "    vq = gradientV(q)\n",
    "    nb_steps = int(T / step)\n",
    "    \n",
    "    for it in range(nb_steps):\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "\n",
    "    return q, -p, np.array(pos), np.array(moms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Illustrate the points proposed by the leapfrog integrator along a trajectory for a 2-dimensional Gaussian distribution. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_density = multi_gauss(np.zeros(2), np.eye(2))\n",
    "gradV = grad(log_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Display several trajectories of the leapfrog integrator for a 2-dimensional Gaussian target distribution. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Display several trajectories of the leapfrog integrator for a target defined as a mixture of 2-dimensional Gaussian distributions. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange> The Hamiltonian loop </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Use the leapfrog function to write a Hamiltonian loop i.e. a Hamiltonian MCMC with a generic target distribution. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamiltonian_monte_carlo(n_samples, log_prob, initial_position, T = 1, step_size = 0.1):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    n_samples: number of samples to return.\n",
    "    log_prob: opposite of the target log probability.\n",
    "    initial_position: a place to start sampling from.\n",
    "    T: length of leapfrog integration.\n",
    "    step_size: step size of the integration scheme.\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    samples: samples produced by the HMC.\n",
    "    sample_positions: positions obtained by the leapfrog integrator at each time step.\n",
    "    sample_momentums: momentums obtained by the leapfrog integrator at each time step.\n",
    "    accepted: array of 0 and 1 to display which proposed moves have been accepted.\n",
    "    \"\"\"\n",
    "    initial_position = np.array(initial_position)\n",
    "    \n",
    "    gradV = grad(log_prob)\n",
    "\n",
    "    samples = [initial_position]\n",
    "    sample_positions, sample_momentums = [], []\n",
    "    accepted = []\n",
    "\n",
    "    size = (n_samples,) + initial_position.shape[:1]\n",
    "    \n",
    "    # all momentums\n",
    "    momentum = st.norm(0, 1).rvs(size)\n",
    "    \n",
    "    for p0 in tqdm(momentum):\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "\n",
    "    return (np.array(samples[1:]),np.array(sample_positions),np.array(sample_momentums),np.array(accepted),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Display several trajectories of the HMC loop for a Gaussian target distribution </font>\n",
    "\n",
    "<font color=darkred>i) Illustrate the influence of the total length of the leapfrog integrator. </font>\n",
    "\n",
    "<font color=darkred>ii) Illustrate the influence of the step size of the leapfrog integrator. </font>\n",
    "\n",
    "<font color=darkred>iii) Compare the performance with a HM algorithm. </font>\n",
    "\n",
    "<font color=darkred>iv) Compare the performance with a MALA algorithm. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Display several trajectories of the HMC algorithm for a target defined as a mixture of Gaussian distributions. </font>\n",
    "\n",
    "<font color=darkred>i) Illustrate the influence of the total length of the leapfrog integrator. </font>\n",
    "\n",
    "<font color=darkred>ii) Illustrate the influence of the step size of the leapfrog integrator. </font>\n",
    "\n",
    "<font color=darkred>iii) Compare the performance with a HM algorithm. </font>\n",
    "\n",
    "<font color=darkred>iv) Compare the performance with a MALA algorithm. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange> Parameters tuning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Analyze the optimal tuning of the HMC parameters: variance matrix of the momentums, step-size, length of the symplectic integration...</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
