{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkcyan> Estimation des paramètres de la loi Gamma </font>\n",
    "\n",
    "La densité de la  loi Gamma $\\Gamma(a,b)$ est donnée sur $\\mathbb{R}_+^*$ par\n",
    "$$f_{(a,b)}: x \\mapsto \\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx},$$\n",
    "avec des paramètres $a>0$ et $b>0$.  \n",
    "\n",
    "Soit $\\mathbf x_{1:n}=(x_1,\\dots,x_n)$ une réalisation de $\\mathbf X_{1:n}=(X_1,\\dots,X_n)$ où les $X_i$ sont  i.i.d. de la loi Gamma $\\Gamma(a,b)$.\n",
    "\n",
    "L'objectif de ce TP est l'estimation des paramètres $a$ et $b$ à partir des observations  $\\mathbf x_{1:n}$.\n",
    "\n",
    "\n",
    "#### <font color=darkorange> Estimateur du maximum de vraisemblance </font>\n",
    "\n",
    "La logvraisemblance de ce modèle s'écrit\n",
    "$$\\ell_n: (a,b) \\mapsto \\sum_{i=1}^n\\log f_{(a,b)}(x_i)= na\\log(b) -n\\log(\\Gamma(a)) +(a-1)\\sum_{i=1}^n\\log(x_i) -b\\sum_{i=1}^nx_i.$$\n",
    "\n",
    "L'estimateur du maximum de vraisemblance de $a$ et $b$ est donné par\n",
    "$$(\\hat a_n, \\hat b_n) = \\mathrm{argmax}_{a>0,b>0}\\{\\ell_n(a,b)\\}.$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set_theme() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "- Ecrire une fonction qui évalue la logvraisemblance $\\ell(a,b)$. Elle prend en argument des valeurs pour les paramètres $a$ et $b$ et les observations $\\mathbf x=(x_1,\\dots,x_n)$. Pour calculer la fonction Gamma on pourra utiliser `math.gamma`. On choisira pour commencer l'exemple `a=2` et `b=0.5`.\n",
    "- Vérifiez votre calcul avec la méthode `log_pdf` de `from scipy.stats import gamma`\n",
    "- Générez un échantillon de taille `n=5000` avec les paramètres précédents et tracez l'histogramme correspondant que vous comparerez à la densité de la loi Gamma (utilisez la méthode `gamma.pdf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_logpdf(x,a,beta):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    x: samples\n",
    "    a: shape parameter of the Gamma distribution\n",
    "    beta: inverse of the scale parameter of the Gamma distribution\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    logp: loglikelihood of the samples\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    \n",
    "    logp = n*a*np.log(beta) \n",
    "    logp = logp - n * np.log(math.gamma(a)) + (a-1)*np.sum(np.log(x)) - beta*np.sum(x)\n",
    "    \n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "beta = 0.5\n",
    "scale = 1./beta\n",
    "samples = np.random.gamma(a, scale, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_logpdf(samples,a,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "np.sum(gamma.logpdf(samples,a, loc=0, scale=1.0/beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, ignored = plt.hist(samples, 50, density=True)\n",
    "\n",
    "x = np.linspace(gamma.ppf(0.001,a, loc=0, scale=1.0/beta),\n",
    "                gamma.ppf(0.999,a, loc=0, scale=1.0/beta), 100)\n",
    "plt.plot(x, gamma.pdf(x,a, loc=0, scale=1.0/beta), 'r-', lw=2);\n",
    "\n",
    "plt.title('Gamma distribution')\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange> Approximation de l'estimateur du maximum de vraisemblance par la méthode de Newton-Raphson </font>\n",
    "On peut montrer que l'EMV n'admet pas d'expression explicite. En revanche, on peut recourir à la méthode de Newton-Raphson pour l'approcher. Plus précisément, on peut l'appliquer afin de trouver des points critiques de $(a,b)\\mapsto \\ell(a,b)$. \n",
    "\n",
    "#### Une mise à jour de Newton-Raphson\n",
    "Rappelons que la méthode de Newton-Raphson est un algorithme itératif. Notons  $(a_t,b_t)^\\top$, $t\\geq 0$, l'estimateur courant des paramètres, $(a_0,b_0)^\\top$ étant souvent choisi aléatoirement. Une itération de l'algorithme consiste à calculer les nouvelles valeurs des paramètres $(a_{t+1},b_{t+1})^\\top$ par la formule\n",
    "$$(a_{t+1},b_{t+1})^\\top = (a_{t},b_{t})^\\top-[H(a_{t},b_{t})]^{-1}\\nabla\\ell(a_{t},b_{t}),\\qquad\\qquad(*)$$\n",
    "où \n",
    "$H$ désigne la matrice Hessienne de $\\ell$.\n",
    "Pour le gradient et l'inverse de la Hessienne on obtient (voir TD pour les calculs détaillés) \n",
    "\\begin{align*}\n",
    "\\nabla\\ell(a,b) &= \\left(\\frac\\partial{\\partial a} \\ell(a,b),\\frac\\partial{\\partial b} \\ell(a,b)\\right)^\\top\n",
    "=\\left(\n",
    "n\\log b- n (\\log\\Gamma)'(a)+\\sum_{i=1}^n\\log x_i,\\quad\n",
    "\\frac{na}b-\\sum_{i=1}^n x_i\n",
    "\\right)^\\top\n",
    "\\end{align*}\n",
    "et\n",
    "\\begin{align*}\n",
    "[H(a,b)]^{-1}\n",
    "=\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^2}{\\partial^2 a} \\ell(a,b)&\\frac{\\partial^2}{\\partial a\\partial b} \\ell(a,b)\\\\\n",
    "\\frac{\\partial^2}{\\partial a\\partial b} \\ell(a,b)&\\frac{\\partial^2}{\\partial^2 b} \\ell(a,b)\n",
    "\\end{array}\\right)^{-1}\n",
    "&=\n",
    " \\frac1{n\\left(1-a(\\log\\Gamma)''(a)\\right)}\n",
    "\\left(\\begin{array}{cc}\n",
    "a&b\\\\\n",
    "b&b^2(\\log\\Gamma)''(a)\n",
    "\\end{array}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "#### Question 2\n",
    "Nous allons écrire une fonction qui effectue une mise à jour des paramètres. Cette fonction prend en argument les valeurs actuelles des paramètres, $a_{t}$ et  $b_{t}$, ainsi que les données. Elle renvoie les nouvelles valeurs des paramètres $a_{t+1}$ et  $b_{t+1}$ obtenues par $(*)$. \n",
    "On peut calculer directement les dérivées de $\\log\\Gamma$ (voir par exemple https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.polygamma.html). Une alternative est d'utiliser `autograd` grâce à `import autograd` afin de différencier automatiquement toute fonction. On pourra ainsi utiliser la version adaptée de la fonction Gamma : `from autograd.scipy.special import gamma as agamma`.\n",
    "- Ecrire une fonction `gamma_logpdf` qui prend en entrée des échantillons et qui renvoie la fonction logvraisemblance associée à ces échantillons. `gamma_logpdf` renvoie donc une fonction et non l'évaluation d'une fonction en point. Vérifier la validité de votre fonction en l'évaluant sur le même échantillon que précédemment.\n",
    "- En utilisant `autograd` écrire une fonction `NR_update` qui effectue une mise à jour des paramètres.\n",
    "\n",
    "Une aide pour l'utilisation de `autograd` peut être trouvée ici https://github.com/HIPS/autograd/blob/master/docs/tutorial.md (ce n'est pas crucial pour ce TP mais vous y trouverez des éléments supplémentaires utiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version avec autograd\n",
    "# On utilise la version autograd de la fonction Gamma afin de pouvoir \n",
    "# différencier automatiquement la logvraisemblance\n",
    "import autograd.numpy as np\n",
    "import autograd\n",
    "from autograd.scipy.special import gamma as agamma\n",
    "def gamma_logpdf(x):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    x: samples\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    logp: loglikelihood function of the samples\n",
    "    \"\"\"\n",
    "    def logp(params):\n",
    "        n = x.shape[0]\n",
    "        a = params[0]\n",
    "        b = params[1]\n",
    "        \n",
    "        loglik = n*a*np.log(b) \n",
    "        loglik = loglik - n * np.log(agamma(a)) + (a-1)*np.sum(np.log(x)) - b*np.sum(x)\n",
    "    \n",
    "        return loglik\n",
    "    \n",
    "    return logp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammalpdf = gamma_logpdf(samples)\n",
    "gammalpdf([a,beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NR_update(f, x0):\n",
    "    g = autograd.jacobian(f)\n",
    "    h = autograd.hessian(f)\n",
    "    \n",
    "    delta = np.linalg.solve(h(x0), g(x0))\n",
    "    x = x0 - delta\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation, Itérations & critère d'arrêt\n",
    "\n",
    "- La méthode de Newton nécessite un point d'initialisation $(a_{0},b_{0})^\\top$. C'est à l'utilisateur de le choisir. Nous verrons l'importance d'un bon choix du point initial.\n",
    "\n",
    "- La méthode de Newton-Raphson consiste à répéter les mises à jour décrites ci-dessus pour $t\\geq 0$ par une boucle jusqu'à convergence. Il nous faut alors un critère d'arrêt qui vérifie après chaque itération si l'algorithme a convergé. \n",
    "\n",
    "- Plusieurs critères d'arrêt sont envisageable. On commence simplement par fixer un nombre maximum d'itérations.\n",
    "\n",
    "\n",
    "#### Question 3\n",
    "- Ecrire une fonction, nommée `mle_gamma()`, qui approche l'estimateur du maximum de vraisemblance en utilisant la méthode de Newton-Raphson. Elle prend en argument les données et les valeurs initiales $a_{0}$ et $b_{0}$. En partant de ces valeurs initiales on itère des mises à jour des paramètres selon la méthode de Newton jusqu'à convergence. Utilisez comme critère d'arrêt le nombre `max_iter` maximum d'itérations. La fonction renvoie la suite des estimations et de leur logvraisemblance au fil des itérations.\n",
    "\n",
    "- Tracez la suite des estimations et de leur logvraisemblance au fil des itérations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_gamma(f, x0, maxiter=50):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    f: function to be optimized\n",
    "    x0: initial parameter estimate\n",
    "    maxiter: maximum number of updates\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    a, b: sequences of estimators\n",
    "    loglik: sequence of loglikelihood along iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    a = [x0[0]]\n",
    "    b = [x0[1]]\n",
    "    loglik = [f(x0)]\n",
    "    for _ in range(maxiter):\n",
    "        x0 = NR_update(f, x0)\n",
    "        \n",
    "        # Condition for Question 5\n",
    "        if x0[0]<0 or x0[1]<0:\n",
    "            return a, b, loglik\n",
    "\n",
    "        loglik.append(f(x0))\n",
    "        a.append(x0[0])\n",
    "        b.append(x0[1])\n",
    "        \n",
    "    return a, b, loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1,0.1])\n",
    "f = gamma_logpdf(samples)\n",
    "a, b, loglik = mle_gamma(f, x0)\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8, 8))\n",
    "\n",
    "fig.suptitle('Approximation of the MLE for the Gamma distribution')\n",
    "axs[0].plot(loglik, 'r-', lw=2, label = 'Loglikelihood');\n",
    "axs[0].tick_params(labelright=True)\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(a, 'b--', lw=1, label = 'a');\n",
    "axs[1].plot(b, 'r--', lw=1, label = 'b');\n",
    "axs[1].tick_params(labelright=True)\n",
    "axs[1].grid(True)\n",
    "axs[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Testez différentes valeurs de paramètres $a$ et $b$ ainsi que différentes tailles d'échantillons et différentes estimations initiales. Pour commencer, initialisez toujours avec les vraies valeurs des paramètres. Dans ce cas, l'algorithme doit converger très rapidement. Si ce n'est pas le cas, votre programme contient des erreurs qu'il faut corriger !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true parameters\n",
    "a_star, b_star = 2, 0.5\n",
    "scale_star = 1.0/b_star\n",
    "# number of initial points\n",
    "p = 5\n",
    "# different sample sizes\n",
    "ns = np.array([100,1000,2500])\n",
    "\n",
    "a = np.linspace(0.1, 3, p)\n",
    "b = np.linspace(0.1, 1, p)\n",
    "\n",
    "init_params = np.array([a,b]).T\n",
    "# ou bien\n",
    "# init_params = np.column_stack((a,b))\n",
    "\n",
    "fig, axs = plt.subplots(3, figsize=(8, 12))\n",
    "for i, n in enumerate(ns):\n",
    "    samples = np.random.gamma(a_star, scale_star, n)\n",
    "    f = gamma_logpdf(samples)\n",
    "    axs[i].tick_params(labelright=True)\n",
    "    axs[i].grid(True)\n",
    "    axs[i].axis(ymin=0.0,ymax=2.5)\n",
    "    axs[i].set_title(f'Estimated parameters with n = {n} samples')\n",
    "    for j in range(p):\n",
    "        x0 = init_params[j]\n",
    "        a, b, loglik = mle_gamma(f, x0, maxiter=20)\n",
    "        axs[i].plot(a, 'b--', lw=1, label=fr'$a_{j}$');\n",
    "        axs[i].plot(b, 'r--', lw=1, label=fr'$b_{j}$');\n",
    "    axs[i].legend(loc=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#### Question 5\n",
    "On observe parfois que les estimations produisent des erreurs. Il arrive que la fonction `NR_update()` renvoie des valeurs négatives pour `a` et/ou `b` (ce qui n'est pas admissible du point de vue d'interprétation de $a$ et $b$ comme paramètres de la loi Gamma, mais la méthode de Newton ne respecte pas le domaine de définition). Dans ce cas, le calcul de la fonction de log-vraisemblance produit des erreurs. Modifiez la fonction `mle_gamma()` en sorte qu'elle arrête l'algorithme dès que `NR_update()` renvoie des valeurs négatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cf code de la fonction NR_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation avec l'estimateur par la méthode des moments\n",
    "\n",
    "Un autre estimateur des paramètres $a$ et $b$ dans ce modèle est donné par l'estimateur par la méthode des moments (EMM). En TD vous montrez que cet estimateur est donné par\n",
    "$$\\tilde a_n = \\frac{\\bar X_n^2}{s_n^2},\\qquad \\tilde b_n = \\frac{\\bar X_n}{s_n^2},$$\n",
    "où $s_n^2=n^{-1}\\sum_{i=1}^n(X_i-\\bar X_n)^2$.\n",
    "\n",
    "\n",
    "\n",
    "#### Question 6\n",
    "Ecrire une fonction nommée `emm_gamma()` qui prend en argument les observations et renvoie l'estimateur par la méthode des moments de $a$ et $b$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise la version autograd de la fonction Gamma afin de pouvoir \n",
    "# différencier automatiquement la logvraisemblance\n",
    "def emm_gamma(x):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    x: samples\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    params: estimators obtained with the methods of moments\n",
    "    \"\"\"\n",
    "    samples_mean = np.mean(x)\n",
    "    samples_std = np.std(x)\n",
    "    \n",
    "    a = samples_mean*samples_mean/(samples_std*samples_std)\n",
    "    b = samples_mean/(samples_std*samples_std)\n",
    "    \n",
    "    params = np.array([a,b])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emm_gamma(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "Etudier de nouveau la convergence de l'algorithme sur des données simulées lors la méthode de Newton-Raphson est initialisée avec l'estimateur des moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true parameters\n",
    "a_star, b_star = 2, 0.5\n",
    "scale_star = 1.0/b_star\n",
    "\n",
    "# different number of samples\n",
    "n = np.array([100,1000,2000])\n",
    "\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid(True)\n",
    "plt.axis(ymin=0.0,ymax=2.5)\n",
    "    \n",
    "for j in range(n.shape[0]):\n",
    "    samples = np.random.gamma(a_star, scale_star, n[j])\n",
    "    f = gamma_logpdf(samples)\n",
    "    theta = emm_gamma(samples)\n",
    "    a, b, loglik = mle_gamma(f, theta)\n",
    "    plt.plot(a, 'b--', lw=1);\n",
    "    plt.plot(b, 'r--', lw=1);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange> Comparaison de l'EMV et de l'EMM </font>\n",
    "On veut comparer les deux estimateurs, EMV et EMM, des paramètres de la loi Gamma. \n",
    "Pour cela nous allons comparer empiriquement les erreurs quadratiques obtenues avec les deux méthodes.\n",
    "\n",
    "\n",
    "#### Question 8\n",
    "Ecrire un programme qui (i) génère un grand nombre de jeux de données de loi Gamma, (ii) évalue les deux estimateurs sur ces données et (iii) propose un boxplot des erreurs quadratiques pour les deux estimateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true parameters\n",
    "a_star, b_star = 2, 0.5\n",
    "scale_star = 1.0/b_star\n",
    "param_star = np.array([a_star,b_star])\n",
    "# number of observations\n",
    "n = 2000 \n",
    "# number of independent repetitions\n",
    "p = 50\n",
    "\n",
    "est_mle = []\n",
    "est_mm = []\n",
    "for _ in range(p):\n",
    "    samples = np.random.gamma(a_star, scale_star, n)\n",
    "    f = gamma_logpdf(samples)\n",
    "    x_emm = emm_gamma(samples)\n",
    "    est_mm = np.append(est_mm,np.dot(x_emm-param_star,x_emm-param_star))\n",
    "    a, b, _ = mle_gamma(f, x_emm)\n",
    "    x_mle = np.array([a[-1],b[-1]])\n",
    "    est_mle = np.append(est_mle,np.dot(x_mle-param_star,x_mle-param_star))\n",
    "plt.boxplot([est_mle,est_mm])\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.xticks([1, 2], ['MLE', 'Moments'],rotation=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
